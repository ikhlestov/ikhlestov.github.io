<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Illarion's Notes</title><link>https://ikhlestov.github.io/</link><description>Notes about AI, machine learning, computer vision and programming.</description><atom:link href="https://ikhlestov.github.io/rss.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 28 Jan 2019 23:13:06 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>RBM based Autoencoders with tensorflow</title><link>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</link><dc:creator>Illarion Khlestov</dc:creator><description>&lt;div&gt;&lt;p&gt;Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in &lt;a class="reference external" href="http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf"&gt;Semantic Hashing&lt;/a&gt; paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I've decided to check this.&lt;/p&gt;
&lt;p&gt;This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader's previous knowledge of tensorflow and machine learning field. All code can be found in &lt;a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow"&gt;this repo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RBMs different from usual neural networks in some ways:&lt;/p&gt;
&lt;p&gt;Neural networks usually perform weight update by Gradient Descent, but RMBs use &lt;strong&gt;Contrastive Divergence&lt;/strong&gt; (which is basically a funky term for "approximate gradient descent" &lt;a class="reference external" href="http://deeplearning.net/tutorial/rbm.html"&gt;link to read&lt;/a&gt;). At a glance, contrastive divergence computes a difference between &lt;strong&gt;positive phase&lt;/strong&gt; (energy of first encoding) and &lt;strong&gt;negative phase&lt;/strong&gt; (energy of the last encoding).&lt;/p&gt;
&lt;pre class="code python"&gt;&lt;a name="rest_code_9b70e63327634f7e8b0243b64969f1c3-1"&gt;&lt;/a&gt;&lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_9b70e63327634f7e8b0243b64969f1c3-2"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;visib_inputs_initial&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;first_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_9b70e63327634f7e8b0243b64969f1c3-3"&gt;&lt;/a&gt;&lt;span class="n"&gt;negative_phase&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;matmul&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;a name="rest_code_9b70e63327634f7e8b0243b64969f1c3-4"&gt;&lt;/a&gt;    &lt;span class="n"&gt;tf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;last_reconstructed_probability&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;last_encoded_probability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;a name="rest_code_9b70e63327634f7e8b0243b64969f1c3-5"&gt;&lt;/a&gt;&lt;span class="n"&gt;contrastive_divergence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;positive_phase&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;negative_phase&lt;/span&gt;
&lt;/pre&gt;&lt;p&gt;Also, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read &lt;a class="reference external" href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/"&gt;here&lt;/a&gt; or &lt;a class="reference external" href="http://rocknrollnerd.github.io/ml/2015/07/18/general-boltzmann-machines.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As prototype one layer tensorflow rbm &lt;a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/models/rbm_models/rbm.py"&gt;implementation&lt;/a&gt; was used. For testing, I've taken well known &lt;a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database"&gt;MNIST&lt;/a&gt; dataset(dataset of handwritten digits).&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/"&gt;Read moreâ€¦&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><guid>https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/</guid><pubDate>Wed, 28 Dec 2016 20:33:15 GMT</pubDate></item></channel></rss>