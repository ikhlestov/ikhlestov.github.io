<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<base href="https://ikhlestov.github.io/pages/ml-bookmarks/">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>ML Bookmarks | Illarion Khlestov Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/ml-bookmarks/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'left', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="ML Bookmarks">
<meta property="og:url" content="https://ikhlestov.github.io/pages/ml-bookmarks/">
<meta property="og:description" content="This is the list of some useful papers or resources with short explanation

Leaning Neural Nets by itself

HYPER NETWORKS This work explores hypernetworks:  an approach of using a small network, also ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-10-11T14:59:07Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../">Blog</a>
                </li>
<li>
<a href="../">Pages</a>
                </li>
<li>
<a href="../../listings/">Listings</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">ML&nbsp;Bookmarks</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This is the list of some useful papers or resources with short&nbsp;explanation</p>
<div class="section" id="leaning-neural-nets-by-itself">
<h2>Leaning Neural Nets by&nbsp;itself</h2>
<ul class="simple">
<li>
<a class="reference external" href="https://arxiv.org/pdf/1609.09106v1.pdf">HYPER NETWORKS</a> This work explores hypernetworks:  an approach of using a small network, also known as a hypernetwork, to generate the weights for a larger&nbsp;network.</li>
<li>
<a class="reference external" href="https://arxiv.org/pdf/1607.01097v1.pdf">AdaNet: Adaptive Structural Learning of Artificial Neural Networks</a>  Our approach simultaneously and adaptively learns both the structure of the network as well as its&nbsp;weights.</li>
<li>
<a class="reference external" href="https://arxiv.org/pdf/1511.08130v2.pdf">A Roadmap towards Machine Intelligence</a>   In this paper, some fundamental properties that intelligent machines should have were proposed, focusing in particular on <em>communication</em> and <em>learning</em>.</li>
</ul>
</div>
<div class="section" id="rnn-seq2seq-and-all-related">
<h2>RNN, seq2seq and all&nbsp;related</h2>
<ul class="simple">
<li>
<a class="reference external" href="https://arxiv.org/pdf/1511.04868v4.pdf">A Neural Transducer</a> Net than can generate predicition as more inputs arrives, without attention&nbsp;mechanism.</li>
<li>
<a class="reference external" href="http://distill.pub/2016/augmented-rnns/">Attention and Augmented Recurrent Neural Networks</a> Explanation of various RNNs complex&nbsp;architectures.</li>
<li>
<a class="reference external" href="https://arxiv.org/pdf/1610.03017v1.pdf">Fully Character-Level Neural Machine Translation without Explicit Segmentation</a>  model that maps a source character sequence to a target character sequence without any segmentation. (CNN + highway +&nbsp;biGRU)</li>
</ul>
</div>
<div class="section" id="reinforcement-learning">
<h2>Reinforcement&nbsp;Learning</h2>
<ul class="simple">
<li>
<a class="reference external" href="https://arxiv.org/pdf/1606.04671.pdf">Progressive Neural Networks</a>  progressive networks approach immune to forgetting and can leverage prior knowledge via lateral connections to previously learned&nbsp;features.</li>
<li>
<a class="reference external" href="https://arxiv.org/pdf/1609.00150v1.pdf">Reward Augmented Maximum Likelihood for Neural Structured Prediction</a> (<a class="reference external" href="https://drive.google.com/file/d/0B3Rdm_P3VbRDVUQ4SVBRYW82dU0/view">Short Summary</a>)This paper presents a simple and computationally efficient approach to incorporate task reward into a  maximum likelihood framework. We establish a connection between the log-likelihood and regularized expected reward objectives, showing that at a zero temperature, they are approximately equivalent in  the vicinity of the  optimal&nbsp;solution.</li>
</ul>
</div>
<div class="section" id="optimization-techniques">
<h2>Optimization&nbsp;Techniques</h2>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/pdf/1409.2329v5.pdf">RECURRENT NEURAL NETWORK&nbsp;REGULARIZATION</a></li>
<li><a class="reference external" href="http://arxiv.org/pdf/1603.05118.pdf">Recurrent Dropout without Memory&nbsp;Loss</a></li>
<li><a class="reference external" href="http://r2rt.com/styles-of-truncated-backpropagation.html">Styles of Truncated&nbsp;Backpropagation</a></li>
</ul>
</div>
<div class="section" id="other-topics">
<h2>Other&nbsp;Topics</h2>
<ul class="simple">
<li>
<a class="reference external" href="http://people.idsia.ch/~rupesh/very_deep_learning/">Highway Networks</a> - list of papers, code,&nbsp;etc.</li>
</ul>
</div>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents Â© 2016         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>