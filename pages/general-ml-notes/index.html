<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<base href="https://ikhlestov.github.io/pages/general-ml-notes/">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>General ML Notes | Illarion Khlestov Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/general-ml-notes/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'left', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="General ML Notes">
<meta property="og:url" content="https://ikhlestov.github.io/pages/general-ml-notes/">
<meta property="og:description" content="This notes based on Neural Networks and Deep Learning
and Coursera ML Courses. They may seems to be some way unstructured, but such structure is useful for me.

Contents:

General Approach
Part I
Eval">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-10-02T23:00:05Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../">Blog</a>
                </li>
<li>
<a href="../">Pages</a>
                </li>
<li>
<a href="../../listings/">Listings</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">General ML&nbsp;Notes</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This notes based on <a class="reference external" href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a>
and <a class="reference external" href="https://www.coursera.org/learn/machine-learning">Coursera ML Courses</a>. They may seems to be some way unstructured, but such structure is useful for&nbsp;me.</p>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents:</p>
<ul class="simple">
<li><a class="reference internal" href="#general-approach" id="id1">General&nbsp;Approach</a></li>
<li><a class="reference internal" href="#part-i" id="id2">Part&nbsp;I</a></li>
<li><a class="reference internal" href="#evaluation-of-algorithm" id="id3">Evaluation of&nbsp;algorithm</a></li>
<li><a class="reference internal" href="#overfiting-and-underfiting" id="id4">Overfiting and&nbsp;underfiting</a></li>
<li><a class="reference internal" href="#unusual-networks-types" id="id5">Unusual networks&nbsp;types</a></li>
</ul>
</div>
<div class="section" id="general-approach">
<h2><a class="toc-backref" href="#id1">General&nbsp;Approach</a></h2>
<ul class="simple">
<li>Define network&nbsp;architecture</li>
<li>Choose right cost&nbsp;function</li>
<li>Calculate gradient descent if&nbsp;necessary</li>
<li>Train, tune&nbsp;hyperparameters.</li>
</ul>
</div>
<div class="section" id="part-i">
<h2><a class="toc-backref" href="#id2">Part&nbsp;I</a></h2>
<p>Sigmoid&nbsp;function:</p>
<div class="math">
\begin{equation*}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation*}
</div>
<p><span class="math">\(\sigma(\infty)\approx 1\)</span>, <span class="math">\(\sigma(-\infty)\approx 0\)</span>,
but note, that <span class="math">\(\sigma(0)=1\)</span></p>
<p>Note: <em>sigmoid function</em> (<span class="math">\(\sigma\)</span>) == <em>logistic function</em>
so <em>sigmoid neurons</em> can be called as <em>logistic neurons</em>.</p>
<p><strong>MLP</strong> is an abbreviation for <em>multilayer&nbsp;perceptrons</em></p>
<p><em>cost</em> fucntion == <em>loss</em> function == <em>objective</em>&nbsp;function.</p>
<p><em>Quadratic cost function</em> (or <em>mean squared error</em>, or just <em>MSE</em>):</p>
<div class="math">
\begin{equation*}
C(w,b)  = \frac{1}{2n}\sum_{n}||y(x) - a||^2
\end{equation*}
</div>
<p>Here,
<em>w</em> denotes the collection of all weights in the network,
<em>b</em> all the biases,
<em>n</em> is the total number of training inputs,
<em>a</em> is the vector of outputs from the network when <em>x</em> is input,
and the sum is over all training inputs, <em>x</em>.</p>
<p>An idea of <em>stochastic gradient descent</em> is to estimate the gradient
<span class="math">\(\nabla C\)</span> by computing <span class="math">\(\nabla Cx\)</span> for a small sample of randomly chosen training inputs,
not for all inputs as usual <em>gradient descent</em> do.
For this stochastic gradient descent take small number of <em>m</em> randomly chosen training inputs.
We&#8217;ll label those random training inputs <span class="math">\(X1,X2,\ldots  ,Xm\)</span> and refer to them as a <em>mini-batch</em>.
So now gradinet can be computed&nbsp;as:</p>
<div class="math">
\begin{equation*}
\nabla C \approx \frac{1}{m}\sum_{j=1}^m \nabla C_{X_j}
\end{equation*}
</div>
</div>
<div class="section" id="evaluation-of-algorithm">
<h2><a class="toc-backref" href="#id3">Evaluation of&nbsp;algorithm</a></h2>
<p>What we should&nbsp;do:</p>
<ol class="arabic simple">
<li>Split the dataset into three portions: train set, validate set and test set, in a proportion&nbsp;3:1:1.</li>
<li>When the number of examples <em>m</em> increase, the cost <span class="math">\({J_{test}}\)</span> increases, while <span class="math">\({J_{val}}\)</span> decrease. When <em>m</em> is very large, if <span class="math">\({J_{test}}\)</span> is about equal to <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large bias(underfiting), while if there is a gap between <span class="math">\({J_{test}}\)</span> and <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large&nbsp;variance(overfiting).</li>
<li>To solve the problem of large bias, you may decrease <span class="math">\({\rm{\lambda }}\)</span> in regularization, while increase it for the problem of large&nbsp;variance.</li>
<li>To evaluate the performance of a classification algorithm, we can use the value: precision, recall and&nbsp;F1.</li>
</ol>
<p>Precision:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalsePositive}}
\end{equation*}
</div>
<p>Recall:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalseNegtive}}
\end{equation*}
</div>
<p>F1:</p>
<div class="math">
\begin{equation*}
\frac{{2*Recall*Precision}}{{Recall + Precision}}
\end{equation*}
</div>
</div>
<div class="section" id="overfiting-and-underfiting">
<h2><a class="toc-backref" href="#id4">Overfiting and&nbsp;underfiting</a></h2>
<p>High <strong>bias</strong> is <strong>underfitting</strong> and high <strong>variance</strong> is <strong>overfitting</strong>.</p>
<p>For understanding what exactly mean <em>Bias</em> and <em>Variance</em> you may check <a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">this</a>
or <a class="reference external" href="http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/">this</a>
cool&nbsp;articles.</p>
<p>Next notes based on awesome Andre Ng <a class="reference external" href="https://www.youtube.com/watch?v=F1ka6a13S9I">lecture</a></p>
<p>During training as usual you split your data on train, validation and test sets.
<em>Note:</em> You should keep your validation/test data the same for model you want to compare.
After measuring errors you can get some results.
In this case difference between <em>human error</em> (how human perform such task) and <em>train error</em> will be <strong>bias</strong>.
On the other hand, difference between <em>train error</em> and <em>validation error</em> will be <strong>variance</strong>.</p>
<object data="../../images/ML_notes/bias_variance_explanation_1.svg" style="width: 320px; height: 120px;" type="image/svg+xml">
bias_variance_explanation_1</object>
<p>In such case you should consider this&nbsp;methods</p>
<object data="../../images/ML_notes/bias_variance_workflow_1.svg" style="width: 443px; height: 402px;" type="image/svg+xml">
bias_variance_workflow_1</object>
<p>Solutions inside blue boxes should be applied as first&nbsp;approach.</p>
<p>But sometimes you may have a lot of data from one domain, but test data comes from another.
In this case validation and test data should be from the same domain.
Also you may consider get validation data also from large domain.
But it should be additional validation(say <em>train-valid</em>).
Let&#8217;s see an&nbsp;example.</p>
<object data="../../images/ML_notes/data_spliting_in_domains.svg" style="width: 473px; height: 93px;" type="image/svg+xml">
data_spliting_in_domains</object>
<p>In this case we receive another correlation between&nbsp;errors:</p>
<object data="../../images/ML_notes/bias_variance_explanation_2.svg" style="width: 453px; height: 166px;" type="image/svg+xml">
bias_variance_explanation_2</object>
<p>And solution algorithm will be a little bit more&nbsp;longer:</p>
<object data="../../images/ML_notes/bias_variance_workflow_2.svg" style="width: 443px; height: 675px;" type="image/svg+xml">
bias_variance_workflow_2</object>
</div>
<div class="section" id="unusual-networks-types">
<h2><a class="toc-backref" href="#id5">Unusual networks&nbsp;types</a></h2>
<p><strong>Highway network</strong> -
Like LSTM networks, utilize a learnable gating mechanism to improve information flow across layers.
More simple - process previous input data to the next layer.
<a class="reference external" href="http://people.idsia.ch/~rupesh/very_deep_learning/">link to papers</a> and
<a class="reference external" href="https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa">tensorflow implementation</a>.&nbsp;Intuition:</p>
<div class="math">
\begin{equation*}
y = H (x ; W_{H} ) * T (x ; W_{T} ) + x * C (x ; W_{C} )
\end{equation*}
</div>
<p>where:</p>
<ul class="simple">
<li>
<em>T</em> is <em>transform&nbsp;gate</em>
</li>
<li>
<em>C</em> is <em>carry&nbsp;gate</em>
</li>
</ul>
<p>Gates express how much of the output is produced by transforming  the  input  and  carrying  it,  respectively.
Sometimes carry gate can be set as <span class="math">\(C = 1 - T\)</span> for&nbsp;simplicity.</p>
</div>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents Â© 2016         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>