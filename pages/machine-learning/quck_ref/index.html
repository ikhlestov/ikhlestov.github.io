<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Quick Reference | Illarion Khlestov Blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/machine-learning/quck_ref/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'center', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="Quick Reference">
<meta property="og:url" content="https://ikhlestov.github.io/pages/machine-learning/quck_ref/">
<meta property="og:description" content="Contents:

Definitions
Math
Matrix multiplication
Logarithms / Exponents
Derivatives


Statistics
Activation functions
Back propagation / Gradient descent
Back propagation
Gradient descent
Stochastic ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2017-03-13T17:08:21Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../../">Blog</a>
                </li>
<li>
<a href="../../">Pages</a>
                </li>
<li>
<a href="../../../listings/">Listings</a>
                </li>
<li>
<a href="../../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">Quick&nbsp;Reference</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents:</p>
<ul class="simple">
<li><a class="reference internal" href="#definitions" id="id1">Definitions</a></li>
<li>
<a class="reference internal" href="#math" id="id2">Math</a><ul>
<li><a class="reference internal" href="#matrix-multiplication" id="id3">Matrix&nbsp;multiplication</a></li>
<li><a class="reference internal" href="#logarithms-exponents" id="id4">Logarithms /&nbsp;Exponents</a></li>
<li><a class="reference internal" href="#derivatives" id="id5">Derivatives</a></li>
</ul>
</li>
<li><a class="reference internal" href="#statistics" id="id6">Statistics</a></li>
<li><a class="reference internal" href="#activation-functions" id="id7">Activation&nbsp;functions</a></li>
<li>
<a class="reference internal" href="#back-propagation-gradient-descent" id="id8">Back propagation / Gradient descent</a><ul>
<li><a class="reference internal" href="#back-propagation" id="id9">Back&nbsp;propagation</a></li>
<li><a class="reference internal" href="#gradient-descent" id="id10">Gradient&nbsp;descent</a></li>
<li><a class="reference internal" href="#stochastic-gradient-descent" id="id11">Stochastic gradient&nbsp;descent</a></li>
</ul>
</li>
<li><a class="reference internal" href="#validation-metrics" id="id12">Validation&nbsp;metrics</a></li>
<li><a class="reference internal" href="#knn-and-k-means" id="id13">kNN and&nbsp;k-means</a></li>
<li><a class="reference internal" href="#covariance-and-correlation" id="id14">Covariance and&nbsp;correlation</a></li>
<li><a class="reference internal" href="#pca" id="id15">PCA</a></li>
<li><a class="reference internal" href="#l1-l2-normalization" id="id16">L1/ L2&nbsp;normalization</a></li>
<li><a class="reference internal" href="#subgradient" id="id17">Subgradient</a></li>
</ul>
</div>
<div class="section" id="definitions">
<h2><a class="toc-backref" href="#id1">Definitions</a></h2>
<dl class="docutils">
<dt>One-shot&nbsp;learning</dt>
<dd>aim to lean not from thousands of examples but from one or only a&nbsp;few.</dd>
<dt>Transfer&nbsp;learning</dt>
<dd>apply already trained model with previous knowledge to the new&nbsp;domain.</dd>
<dt>MLP</dt>
<dd>is an abbreviation for <em>multilayer&nbsp;perceptrons</em>
</dd>
<dt>Computational&nbsp;neuroscience</dt>
<dd>is primarily concerned with building more accurate models of how the brain actually&nbsp;works.</dd>
<dt>Stochastic(non-deterministic)</dt>
<dd>a stochastic event or system is one that is unpredictable because of a random&nbsp;variable.</dd>
<dt>Deterministic</dt>
<dd>is a system in which no randomness is involved in the development of future states of the&nbsp;system.</dd>
<dt>Gradient</dt>
<dd>is the vector of partial derivatives in each&nbsp;dimension.</dd>
<dt>Gradient&nbsp;Descent</dt>
<dd>the procedure of repeatedly evaluating the gradient and then performing a parameter&nbsp;update.</dd>
<dt>Stochastic Gradient&nbsp;Descent</dt>
<dd>depends on source it may mean Gradient Descent with batches, or with only one example(on-line gradient&nbsp;descent).</dd>
<dt>Backpropagation</dt>
<dd>computing the gradient analytically using the chain&nbsp;rule.</dd>
<dt>Chain&nbsp;Rule</dt>
<dd>Gradient expressions may be chained with multiplication of output gradient with local function&nbsp;gradient.</dd>
<dt>Bias</dt>
<dd>is a learner’s tendency to consistently learn the same wrong&nbsp;thing.</dd>
<dt>Variance</dt>
<dd>is the tendency to learn random things irrespective of the real&nbsp;signal.</dd>
</dl>
</div>
<div class="section" id="math">
<h2><a class="toc-backref" href="#id2">Math</a></h2>
<div class="section" id="matrix-multiplication">
<h3><a class="toc-backref" href="#id3">Matrix&nbsp;multiplication</a></h3>
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/780671bdfb3fab93187156d2c226df6fe36746fe" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/780671bdfb3fab93187156d2c226df6fe36746fe"><div class="figure">
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/24e439c972de89fb94297419cab45c58f1a32c43" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/24e439c972de89fb94297419cab45c58f1a32c43">
</div>
<pre class="code python"><a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-1"></a><span class="c1"># shape (2, 3)</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-2"></a><span class="n">A</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-3"></a>     <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-4"></a><span class="c1"># shape (3, 2)</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-5"></a><span class="n">B</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-6"></a>     <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">],</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-7"></a>     <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]]</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-8"></a>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-9"></a><span class="c1"># shape (2, 2)</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-10"></a><span class="n">Z</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">9</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">1</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="mi">12</span><span class="p">],</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-11"></a>     <span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">9</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">8</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="mi">10</span> <span class="o">+</span> <span class="mi">6</span> <span class="o">*</span> <span class="mi">12</span><span class="p">]]</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-12"></a><span class="c1"># multiply row of A by the column of B and sum the result</span>
<a name="rest_code_0e48c2b582ef48ee8fb1381ebc11e900-13"></a><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">B</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>
</pre>
<div class="figure">
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/c6f7cdbe9cd36a436b0e5304ea2b42d640203777" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c6f7cdbe9cd36a436b0e5304ea2b42d640203777">
</div>
<div class="figure">
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f877a81f2a22a2109790c35f6c11bf85af22b82" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f877a81f2a22a2109790c35f6c11bf85af22b82">
</div>
</div>
<div class="section" id="logarithms-exponents">
<h3><a class="toc-backref" href="#id4">Logarithms /&nbsp;Exponents</a></h3>
<div class="math">
\begin{equation*}
\ln (e ^ x) = x
\end{equation*}
</div>
<div class="math">
\begin{equation*}
\log_{a} (a ^ x) = x
\end{equation*}
</div>
<img alt="https://s-media-cache-ak0.pinimg.com/originals/bd/b0/ce/bdb0ce6d9c5a6e2a8006e639ee01202b.jpg" src="https://s-media-cache-ak0.pinimg.com/originals/bd/b0/ce/bdb0ce6d9c5a6e2a8006e639ee01202b.jpg">
</div>
<div class="section" id="derivatives">
<h3><a class="toc-backref" href="#id5">Derivatives</a></h3>
<img alt="http://www.mathspadilla.com/matII/Unit3-Derivatives/deriv11.png" src="http://www.mathspadilla.com/matII/Unit3-Derivatives/deriv11.png"><img alt="https://www.eeweb.com/tools/math-sheets/images/calculus-derivatives-limits.png" src="https://www.eeweb.com/tools/math-sheets/images/calculus-derivatives-limits.png">
</div>
</div>
<div class="section" id="statistics">
<h2><a class="toc-backref" href="#id6">Statistics</a></h2>
<p>standard deviation - square root of variance. <span class="math">\(std = \sqrt{variance}\)</span>. <span class="math">\(\sigma = \sqrt{ \frac{1}{n} \sum_{i=1}^{n}(x_i -&nbsp;x_{mean})^2}\)</span></p>
</div>
<div class="section" id="activation-functions">
<h2><a class="toc-backref" href="#id7">Activation&nbsp;functions</a></h2>
<div class="math">
\begin{equation*}
\textsf{Sigmoid}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
f(x) = \frac{1}{1 + e^{-x}}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
\textsf{Tanh}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
f(x) = \tanh(x) = \frac{2}{1 + e^{-2x}} - 1
\end{equation*}
</div>
<div class="math">
\begin{equation*}
\textsf{Softmax}
\end{equation*}
</div>
<div class="math">
\begin{equation*}
f(x)_i = \frac{e^{x_i}}{\sum_{k=1}^{K} e^{x_k}}
\end{equation*}
</div>
</div>
<div class="section" id="back-propagation-gradient-descent">
<h2><a class="toc-backref" href="#id8">Back propagation / Gradient&nbsp;descent</a></h2>
<div class="section" id="back-propagation">
<h3><a class="toc-backref" href="#id9">Back&nbsp;propagation</a></h3>
<p>Starting with the final output recursively applies the chain rule to compute the gradients of every layer.
For <span class="math">\(L_{i}\)</span> layer backprop can be computed as derivative for every element based on <span class="math">\(L_{i + 1}\)</span> layer backprop&nbsp;output.</p>
</div>
<div class="section" id="gradient-descent">
<h3><a class="toc-backref" href="#id10">Gradient&nbsp;descent</a></h3>
<p>To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point.
Gradient - vector containing all of the partial derivatives. This mean in case while computed derivative for one function input, all other stay the&nbsp;same.</p>
</div>
<div class="section" id="stochastic-gradient-descent">
<h3><a class="toc-backref" href="#id11">Stochastic gradient&nbsp;descent</a></h3>
<p>Perform Gradient Descent only with some part of&nbsp;examples</p>
</div>
</div>
<div class="section" id="validation-metrics">
<h2><a class="toc-backref" href="#id12">Validation&nbsp;metrics</a></h2>
<p>Confusion Matrix - matrix contains True/False&nbsp;positives/negatives.</p>
<p>Precision: <span class="math">\(\frac{{TruePositive}}{{TruePositive + FalsePositive}}\)</span>.
Put another way, it is the number of positive predictions divided by the total number of positive class values predicted.
A low precision can also indicate a large number of False Positives.
<em>How many selected items are relevant</em>.
Also the precision is intuitively the ability of the classifier not to label as positive a sample that is&nbsp;negative.</p>
<p>Recall: <span class="math">\(\frac{{TruePositive}}{{TruePositive + FalseNegtive}}\)</span>.
Put another way it is the number of positive predictions divided by the number of positive class values in the test data.
Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives.
<em>How many relevant items are selected</em>.
Also the recall is intuitively the ability of the classifier to find all the positive&nbsp;samples.</p>
<p>F1 score: <span class="math">\(\frac{{2*Recall*Precision}}{{Recall + Precision}}\)</span> balanced precision and&nbsp;recall.</p>
</div>
<div class="section" id="knn-and-k-means">
<h2><a class="toc-backref" href="#id13">kNN and&nbsp;k-means</a></h2>
<p><strong>kNN(k-nearest neighbors algorithm)</strong> - classification algorithm when class of undefined element will be issued based on classes of K nearest&nbsp;neighbors.</p>
<p><strong>k-means</strong> - clusterization algorithm. Aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the&nbsp;cluster:</p>
<ul class="simple">
<li>Define k&nbsp;clusters</li>
<li>Calculate distance to every&nbsp;point</li>
<li>Assign each pending point to the nearest&nbsp;cluster</li>
<li>Recalculate new clusters&nbsp;centers</li>
<li>Recalculate new distances: <span class="math">\(v_i = (1/c_i) \sum_{j=1}^{c_i} x_i\)</span>, where <span class="math">\(c_i\)</span> represents number of data points in <span class="math">\(i^{th}\)</span>&nbsp;cluster.</li>
<li>If no any points were reassigned - stop&nbsp;iterations</li>
</ul>
</div>
<div class="section" id="covariance-and-correlation">
<h2><a class="toc-backref" href="#id14">Covariance and&nbsp;correlation</a></h2>
<p>Both describe the degree to which two random variables or sets of random variables tend to deviate from their expected values in similar&nbsp;ways.</p>
<p>If <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are two random variables, with means (expected values) <span class="math">\(\mu_X\)</span> and <span class="math">\(\mu_Y\)</span> and standard deviations <span class="math">\(\sigma_X\)</span> and <span class="math">\(\sigma_Y\)</span>, respectively, then their covariance and correlation are as&nbsp;follows:</p>
<div class="figure">
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f6328c2a98e12b74140dfc6fb614f7939e12a1c" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f6328c2a98e12b74140dfc6fb614f7939e12a1c"><p class="caption">Covariance</p>
</div>
<div class="figure">
<img alt="https://wikimedia.org/api/rest_v1/media/math/render/svg/c8ad7d4bca4314703d33deb0245732bcc249dfa4" src="https://wikimedia.org/api/rest_v1/media/math/render/svg/c8ad7d4bca4314703d33deb0245732bcc249dfa4"><p class="caption">Correlation</p>
</div>
<p>where <span class="math">\(E[ ]\)</span> is the expected value, also known as the&nbsp;mean.</p>
</div>
<div class="section" id="pca">
<h2><a class="toc-backref" href="#id15">PCA</a></h2>
<p>Principal components - components with most variation, directions where the data is most spread&nbsp;out.</p>
<p>Eigenvectors and values exist in pairs: every eigenvector has a corresponding eigenvalue. An eigenvector is a direction.
An eigenvalue is a number, telling you how much variance there is in the data in that direction, in the example above the eigenvalue is a number telling us how spread out the data is on the line.
The eigenvector with the highest eigenvalue is therefore the principal&nbsp;component.</p>
<p>In fact the amount of eigenvectors/values that exist equals the number of dimensions the data set&nbsp;has.</p>
<p>Reducing dimension performed by stripping some eigenvectors with small eigenvalues. Only eigenvectors with large eigenvalues&nbsp;remains.</p>
<p>Also Multiple Discriminant Analysis(MDA) approach exist. In MDA we are additionally interested to find the directions that maximize the separation (or discrimination) between different classes (for example, in pattern classification problems where our dataset consists of multiple classes. In contrast two PCA, which ignores the class&nbsp;labels).</p>
<p>PCA step by&nbsp;step:</p>
<ul class="simple">
<li>Compute means of every&nbsp;dimension.</li>
<li>Compute the scatter matrix <span class="math">\(S = \sum\limits_{k=1}^n (\pmb x_k - \pmb m)\;(\pmb x_k - \pmb m)^T\)</span>, where <span class="math">\(\pmb m\)</span> is the mean&nbsp;vector.</li>
<li>Or alternatively compute covariance matrix (numpy.cov function) (a matrix whose element in the i, j position is the covariance between the <span class="math">\(i^{th}\)</span> and <span class="math">\(j^{th}\)</span> elements of a random&nbsp;vector).</li>
<li>Compute eigenvectors/ eigenvalues: <tt class="docutils literal">eig_val_sc, eig_vec_sc = np.linalg.eig(scatter_matrix)</tt><ul>
<li>Eigenvalues <span class="math">\(\alpha\)</span> can be obtained by solving an equation <span class="math">\(|\textbf{A} - \alpha \textbf{I}| = 0\)</span>, where <span class="math">\(\textbf{A}\)</span> is a matrix and <span class="math">\(| |\)</span> means&nbsp;determinant.</li>
<li>Eigenvectors <span class="math">\(\pmb v\)</span> than can be obtained by <span class="math">\((\textbf{A} - \alpha_j \textbf{I})\pmb v_j = 0\)</span>.</li>
</ul>
</li>
<li>We can check correctness of eigenvectors/eigenvalues as <span class="math">\(\pmb\Sigma\pmb{v} = \lambda\pmb{v}\)</span>, where <span class="math">\(\pmb\Sigma\)</span> - covariance matrix, <span class="math">\(\pmb{v}\)</span> - eigenvector, <span class="math">\(\lambda\)</span> -&nbsp;eigenvalue.</li>
<li>Sorting the eigenvectors by decreasing&nbsp;eigenvalues</li>
<li>Choosing k eigenvectors with the largest eigenvalues and receive <span class="math">\(\pmb W\)</span>&nbsp;matrix.</li>
<li>To receive dimension reduction we should only compute <span class="math">\(\pmb y = \pmb W^T \times \pmb&nbsp;x\)</span>
</li>
</ul>
</div>
<div class="section" id="l1-l2-normalization">
<h2><a class="toc-backref" href="#id16">L1/ L2&nbsp;normalization</a></h2>
<p>The idea of regularization is to add an extra term to the cost function, a term called the regularization&nbsp;term.</p>
<p>Regularization term for <span class="math">\(L_p\)</span> norm can be computed as <span class="math">\(||x||_{p}=(\sum_{i}|x_{i}|^{p})^{1/p}\)</span>.</p>
<p>Great explanation can be found <a class="reference external" href="http://stackoverflow.com/questions/32276391/feature-normalization-advantage-of-l2-normalization">on stackoverflow</a>
or <a class="reference external" href="http://stats.stackexchange.com/questions/163388/l2-regularization-is-equivalent-to-gaussian-prior">here</a></p>
</div>
<div class="section" id="subgradient">
<h2><a class="toc-backref" href="#id17">Subgradient</a></h2>
<p>Something used for not differentiable functions. SHould be&nbsp;filled.</p>
</div>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2018         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92406723-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>