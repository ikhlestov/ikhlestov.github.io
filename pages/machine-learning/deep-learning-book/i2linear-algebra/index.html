<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>I.2.Linear Algebra | Illarion&#8217;s Notes</title>
<link href="../../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/machine-learning/deep-learning-book/i2linear-algebra/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'center', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion's Notes">
<meta property="og:title" content="I.2.Linear Algebra">
<meta property="og:url" content="https://ikhlestov.github.io/pages/machine-learning/deep-learning-book/i2linear-algebra/">
<meta property="og:description" content="Contents:

Matrices
Vectors
Norms
Special Kinds of Matrices and Vectors
Eigendecomposition
Singular Value Decomposition
Moore-Penrose pseudoinverse
The Trace Operator
The Determinant


If you need jus">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-12-15T21:55:43Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="https://ikhlestov.github.io/">

            <span id="blog-title">Illarion&#8217;s Notes</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../../../" class="nav-link">About</a>
                </li>
<li class="nav-item">
<a href="../../../../posts/" class="nav-link">Blog&nbsp;Posts</a>
                </li>
<li class="nav-item">
<a href="../../../" class="nav-link">Pages</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <nav class="breadcrumbs"><ul class="breadcrumb">
<li class="breadcrumb-item"><a href="../../../">pages</a></li>
                <li class="breadcrumb-item"><a href="../../">machine-learning</a></li>
                <li class="breadcrumb-item"><a href="../">deep-learning-book</a></li>
                <li class="breadcrumb-item active">i2linear-algebra</li>
</ul></nav><div class="body-content">
        <!--Body content-->
        
<article class="post-text storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">I.2.Linear&nbsp;Algebra</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents:</p>
<ul class="simple">
<li><a class="reference internal" href="#matrices" id="id1">Matrices</a></li>
<li><a class="reference internal" href="#vectors" id="id2">Vectors</a></li>
<li><a class="reference internal" href="#norms" id="id3">Norms</a></li>
<li><a class="reference internal" href="#special-kinds-of-matrices-and-vectors" id="id4">Special Kinds of Matrices and&nbsp;Vectors</a></li>
<li><a class="reference internal" href="#eigendecomposition" id="id5">Eigendecomposition</a></li>
<li><a class="reference internal" href="#singular-value-decomposition" id="id6">Singular Value&nbsp;Decomposition</a></li>
<li><a class="reference internal" href="#moore-penrose-pseudoinverse" id="id7">Moore-Penrose&nbsp;pseudoinverse</a></li>
<li><a class="reference internal" href="#the-trace-operator" id="id8">The Trace&nbsp;Operator</a></li>
<li><a class="reference internal" href="#the-determinant" id="id9">The&nbsp;Determinant</a></li>
</ul>
</div>
<p>If you need just quick ref - see <a class="reference external" href="http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf">The Matrix CookBook</a>.
For full book about linear algebra - <a class="reference external" href="https://cosmathclub.files.wordpress.com/2014/10/georgi-shilov-linear-algebra4.pdf">Shilov 1977 linear&nbsp;algebra</a></p>
<dl class="docutils">
<dt>Scalars</dt>
<dd>is just a single&nbsp;number.</dd>
<dt>Vectors</dt>
<dd>is an array of&nbsp;numbers.</dd>
<dt>Matrices</dt>
<dd>is a 2-D array of numbers, so each element is identified by two indices instead of just&nbsp;one.</dd>
<dt>Tensors</dt>
<dd>an array with more than two&nbsp;axes.</dd>
</dl>
<div class="section" id="matrices">
<h2><a class="toc-backref" href="#id1">Matrices</a></h2>
<p>The product operation of matrices(<strong>matrix product</strong>) <span class="math">\(C = AB\)</span> is defined as <span class="math">\(C_{i,j} = \sum_{k} A_{i,k} B_{j,k}\)</span>.</p>
<p>Matrix containing the product of the individual elements called the <strong>element-wise product</strong> or <strong>Hadamard product</strong>, and is denoted as <span class="math">\(A \odot B\)</span>.</p>
<p>Matrix multiplication is distributive <span class="math">\(A(B + C) = AB + AC\)</span>.</p>
<p>Matrix multiplication is associative also <span class="math">\(A(BC) = (AB)C\)</span>.</p>
<p>Transpose matrix product <span class="math">\((AB)^{T} = B^{T} A^{T}\)</span>.</p>
<p>An <strong>identity matrix</strong> is a matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves
<em>n</em>-dimensional vectors as <span class="math">\(I_{n}\)</span>.</p>
<div class="math">
\begin{equation*}
\forall x \in \mathbb{R}, I_{n}x = x
\end{equation*}
</div>
<p>All the entries along the main diagonal in identity matrix is 1, while all other entries are&nbsp;zero.</p>
<p>The <strong>matrix inverse</strong> of <span class="math">\(A\)</span> is denoted as <span class="math">\(A^{-1}\)</span> , and it is defined as the matrix such that <span class="math">\(A A^{-1} = I_{n}\)</span>.</p>
</div>
<div class="section" id="vectors">
<h2><a class="toc-backref" href="#id2">Vectors</a></h2>
<dl class="docutils">
<dt>Origin</dt>
<dd>the point specified by the vector of all&nbsp;zeros.</dd>
<dt>Linear&nbsp;combination</dt>
<dd>of some set of vectors <span class="math">\({ v^{1} , &#8230; , v^{n} }\)</span> is given by multiplying each vector <span class="math">\(v^{i}\)</span> by a corresponding scalar coefficient and adding the results: <span class="math">\(\sum_{i}c_{i}v^{i}\)</span>.</dd>
<dt>Span</dt>
<dd>of a set of vectors is the set of all points obtainable by linear combination of the original&nbsp;vectors.</dd>
</dl>
<p>A set of vectors is <strong>linearly independent</strong> if no vector in the set is a linear combination of the other&nbsp;vectors.</p>
<p>Only <strong>square</strong> matrix with linearly independent columns have <strong>determinant</strong>.</p>
<p>A square matrix with linearly dependent columns is known as <strong>singular</strong>.</p>
<p>For square matrices the left inverse and right inverse are equal <span class="math">\(AA^{-1}=I\)</span>.</p>
</div>
<div class="section" id="norms">
<h2><a class="toc-backref" href="#id3">Norms</a></h2>
<dl class="docutils">
<dt>Norms</dt>
<dd>are functions mapping vectors to non-negative values. Can be threated as size of the&nbsp;vector.</dd>
</dl>
<p><span class="math">\(L^{p}\)</span> norm is given by <span class="math">\(||x||_{p}=(\sum_{i}|x_{i}|^{p})^{1/p}\)</span>, for <span class="math">\(p \in \mathbb{R}, p \geq 1\)</span>.</p>
<dl class="docutils">
<dt>Euclidean&nbsp;norm</dt>
<dd>the <span class="math">\(L^{2}\)</span> norm, with <span class="math">\(p = 2\)</span>.</dd>
</dl>
<p>Squared <span class="math">\(L^{2}\)</span> norm can be calculated simply as <span class="math">\(x^{T}x\)</span>.</p>
<p><span class="math">\(L^{2}\)</span> norm may be undesirable because it increases very slowly near the origin.
In these cases, we turn to a function that grows at the same rate in all locations, but retains mathematical simplicity: the <span class="math">\(L^{1}\)</span> norm.
The <span class="math">\(L^{1}\)</span> norm is commonly used in machine learning when the difference between zero and nonzero elements is very&nbsp;important.</p>
<dl class="docutils">
<dt>
<span class="math">\(L^{1}\)</span>&nbsp;norm</dt>
<dd><span class="math">\(||x||_{1}=\sum_i|x_{i}\)</span></dd>
</dl>
<p>The <span class="math">\(L^{\infty}\)</span> norm, also known as the <strong>max norm</strong>. This norm simplifies to the absolute value of the element with the largest magnitude in the&nbsp;vector.</p>
<dl class="docutils">
<dt>
<span class="math">\(L^{\infty}\)</span>&nbsp;norm</dt>
<dd><span class="math">\(||x||_{\infty} =&nbsp;max_{i}|xi|\)</span></dd>
</dl>
<p>Most common way to measure the size of a matrix this is <strong>Frobenius norm</strong> which is analogous to the <span class="math">\(L^{2}\)</span> norm of a&nbsp;vector.</p>
<dl class="docutils">
<dt>Frobenius&nbsp;norm</dt>
<dd><span class="math">\(||A||_{F}=\sqrt{\sum_{i,j}A^{2}_{i,j}}\)</span></dd>
</dl>
<p>The dot product of two vectors can be rewritten in terms of&nbsp;norms.</p>
<dl class="docutils">
<dt>Dot&nbsp;product</dt>
<dd><span class="math">\(x^{T}y=||x||_{2}||y||_{2}\cos\theta\)</span></dd>
</dl>
<p>where <span class="math">\(\theta\)</span> is the angle between <span class="math">\(x\)</span> and <span class="math">\(y\)</span>.</p>
</div>
<div class="section" id="special-kinds-of-matrices-and-vectors">
<h2><a class="toc-backref" href="#id4">Special Kinds of Matrices and&nbsp;Vectors</a></h2>
<p><strong>Diagonal</strong> matrices consist mostly of zeros and have non-zero entries only alongthe main diagonal.
We write <span class="math">\(diag(v)\)</span> to denote a square diagonal matrix whose diagonal entries are given by the entries of the vector <span class="math">\(v\)</span>.
To compute <span class="math">\(diag(v)x\)</span>, we only need to scale each element <span class="math">\(x_i\)</span> by <span class="math">\(v_i\)</span>. In other words, <span class="math">\(diag(v)x = x \odot y\)</span>.
The inverse exists only if every diagonal entry is nonzero, and in that case, <span class="math">\(diag(v)^{-1} = diag([1/v_1, &#8230;, 1/v_n]^T)\)</span>.</p>
<p>A <strong>symmetric matrix</strong> is any matrix that is equal to its own transpose: <span class="math">\(A = A^T\)</span>.</p>
<p>A <strong>unit vector</strong> is a vector with unit norm: <span class="math">\(||x||_2 = 1\)</span>.</p>
<p>A vector <span class="math">\(x\)</span> and a vector <span class="math">\(y\)</span> are <strong>orthogonal</strong> to each other if <span class="math">\(x^Ty = 0\)</span>.
In <span class="math">\(\mathbb{R}^{n}\)</span>, at most <span class="math">\(n\)</span> vectors may be mutually orthogonal with nonzero norm.
If the vectors are not only orthogonal but also have unit norm, we call them <strong>orthonormal</strong>.</p>
<p>An <strong>orthogonal matrix</strong> is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal: <span class="math">\(A^TA = AA^T = I\)</span>.
This implies that <span class="math">\(A^{-1} = A^T\)</span>.</p>
</div>
<div class="section" id="eigendecomposition">
<h2><a class="toc-backref" href="#id5">Eigendecomposition</a></h2>
<p><strong>Edeigen-decomposition</strong> decompose a matrix into a set of eigenvectors and eigenvalues.
An <strong>eigenvector</strong> of a square matrix <span class="math">\(\pmb A\)</span> is a non-zero vector <span class="math">\(\pmb v\)</span> such that multiplication by <span class="math">\(\pmb A\)</span> alters only the scale of <span class="math">\(\pmb v\)</span>:</p>
<div class="math">
\begin{equation*}
\pmb{Av} = \lambda \pmb{v}.
\end{equation*}
</div>
<p>The scalar <span class="math">\(\lambda\)</span> is known as the <strong>eigenvalue</strong> corresponding to this eigenvector.
If <span class="math">\(\pmb v\)</span> is an eigenvector of <span class="math">\(\pmb A\)</span>, then so is any rescaled vector <span class="math">\(\pmb{sv}\)</span> for <span class="math">\(\pmb{s} \in \mathbb{R}, s \neq 0\)</span>.
Moreover, <span class="math">\(\pmb{sv}\)</span> still has the same eigenvalue.
For this reason, we usually only look for unit&nbsp;eigenvectors.</p>
<p>Suppose that a matrix <span class="math">\(\pmb A\)</span> has <span class="math">\(n\)</span> linearly independent eigenvectors,
<span class="math">\({v^{(1)}, &#8230; ,v^{(n)}}\)</span>, with corresponding eigenvalues
<span class="math">\({\lambda_1, &#8230; , \lambda_n}\)</span>.
We may concatenate all of the eigenvectors to form a matrix <span class="math">\(\pmb V\)</span> with one eigenvector per column: <span class="math">\(\pmb V = [v^{(1)}, &#8230; ,v^{(n)}]\)</span>.
Likewise, we can concatenate the eigenvalues to form a vector
<span class="math">\(\pmb{\lambda}= [\lambda_1, &#8230; ,\lambda_n]^T\)</span>.
The eigendecomposition of <span class="math">\(\pmb A\)</span> is then given&nbsp;by:</p>
<div class="math">
\begin{equation*}
\pmb A = \pmb V diag(\pmb{\lambda}) \pmb V^{-1}
\end{equation*}
</div>
<p>Not every matrix can be decomposed into eigenvalues and eigenvectors.
Every real symmetricmatrix can be decomposed into an expression using only real-valued eigenvectors and&nbsp;eigenvalues:</p>
<div class="math">
\begin{equation*}
\pmb A = \pmb{Q \wedge Q}^T
\end{equation*}
</div>
<p>where <span class="math">\(\pmb Q\)</span> is an orthogonal matrix composed of eigenvectors of <span class="math">\(\pmb A\)</span>,
and <span class="math">\(\pmb{\wedge}\)</span> is a diagonal matrix.
The eigenvalue <span class="math">\(\wedge_{i,i}\)</span> is associated with the eigenvector in columni of
<span class="math">\(\pmb{Q}\)</span>, denoted as <span class="math">\(\pmb{Q}_{:,i}\)</span>.
Because <span class="math">\(\pmb{Q}\)</span> is an orthogonal matrix, we can think of <span class="math">\(\pmb{A}\)</span> as scaling space by <span class="math">\(\lambda_i\)</span> in direction <span class="math">\(\pmb{v}^{(i)}\)</span>.</p>
<p>While any real symmetric matrix <span class="math">\(\pmb{A}\)</span> is guaranteed to have an eigendecomposition, the eigendecomposition may not be unique.
If any two or more eigenvectors share the same eigenvalue, then any set of orthogonal vectors lying in their span are also eigenvectors with that eigenvalue, and we could equivalently choose a <span class="math">\(\pmb{Q}\)</span> using those eigenvectors instead.
By convention, we usually sort the entries of <span class="math">\(\pmb{\wedge}\)</span> in descending order.
Under this convention, the eigendecomposition is unique only if all of the eigenvalues are&nbsp;unique.</p>
<p>The matrix is singular if and only if any of the eigenvalues are zero.
The eigendecomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form <span class="math">\(f(\pmb{x}) = \pmb{x}^T \pmb{Ax}\)</span>
subject to <span class="math">\(||\pmb{x}||_2 = 1\)</span>.
Whenever <span class="math">\(\pmb x\)</span> is equal to an eigenvector of <span class="math">\(\pmb A\)</span>, <span class="math">\(f\)</span> takes on the value of the corresponding eigenvalue.
The maximum value of <span class="math">\(f\)</span> within the constraint region is the maximum eigenvalue and its minimum value within the constraint region is the minimum&nbsp;eigenvalue.</p>
<p>A matrix whose eigenvalues are all positive is called <strong>positive definite</strong>.
A matrix whose eigenvalues are all positive or zero-valued is called <strong>positive semidefinite</strong>.
Positive semidefinite matrices are interesting because they guarantee that
<span class="math">\(\forall \pmb x, \pmb{x}^T \pmb{Ax} \geq 0\)</span>.
Positive definite matrices additionally guarantee that
<span class="math">\(\pmb{x}^T \pmb{Ax} = 0 \Rightarrow \pmb x = 0\)</span>.</p>
</div>
<div class="section" id="singular-value-decomposition">
<h2><a class="toc-backref" href="#id6">Singular Value&nbsp;Decomposition</a></h2>
<p>The <strong>singular value decomposition</strong> (SVD) provides another way to factorize a matrix, into <strong>singular vectors</strong> and <strong>singular values</strong>.
Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition.
For example, if a matrix is not square, the eigendecomposition is not deﬁned, and we must use a singular value decomposition&nbsp;instead.</p>
<p>The singular value decomposition is similar to eigendecomposition, except this time we will write <span class="math">\(\pmb A\)</span> as a product of three&nbsp;matrices:</p>
<div class="math">
\begin{equation*}
\pmb A = \pmb{U DV}^T
\end{equation*}
</div>
<p>Suppose that <span class="math">\(\pmb A\)</span> is an <em>m</em> x <em>n</em> matrix.
Then <span class="math">\(\pmb U\)</span> is deﬁned to be an <em>m</em> x <em>m</em> matrix,
<span class="math">\(\pmb D\)</span> to be an <em>m</em> x <em>n</em> matrix,
and <span class="math">\(\pmb V\)</span> to be an <em>n</em> x <em>n</em>&nbsp;matrix.</p>
<p>Each of these matrices is defined to have a special structure.
The matrices <span class="math">\(\pmb U\)</span> and <span class="math">\(\pmb V\)</span> are both defined to be orthogonal matrices.
The matrix <span class="math">\(\pmb D\)</span> is defined to bea diagonal matrix.
Note that <span class="math">\(\pmb D\)</span> is not necessarily&nbsp;square.</p>
<p>The elements along the diagonal of <span class="math">\(\pmb D\)</span> are known as the <strong>singular values</strong> of the matrix <span class="math">\(\pmb A\)</span> .
The columns of <span class="math">\(\pmb U\)</span> are known as the <strong>left-singular vectors</strong>.
The columns of <span class="math">\(\pmb V\)</span> are known as as the <strong>right-singular vectors</strong>.</p>
<p>We can actually interpret the singular value decomposition of <span class="math">\(\pmb A\)</span> in terms of the eigendecomposition of functions of <span class="math">\(\pmb A\)</span>.
The left-singular vectors of <span class="math">\(\pmb A\)</span> are theeigenvectors of <span class="math">\(\pmb{AA}^T\)</span>.
The right-singular vectors of <span class="math">\(\pmb A\)</span> are the eigenvectors of
<span class="math">\(\pmb{A}^T\pmb{A}\)</span> .
The non-zero singular values of <span class="math">\(\pmb A\)</span> are the square roots of the eigenvalues of <span class="math">\(\pmb{A}^T\pmb{A}\)</span> . The same is true for <span class="math">\(\pmb{AA}^T\)</span>&nbsp;.</p>
</div>
<div class="section" id="moore-penrose-pseudoinverse">
<h2><a class="toc-backref" href="#id7">Moore-Penrose&nbsp;pseudoinverse</a></h2>
<p>Suppose we want to make a left-inverse <span class="math">\(B\)</span> of a matrix <span class="math">\(A\)</span>, so that we can solve a linear equation <span class="math">\(Ax = y\)</span> by left-multiplying each side to obtain <span class="math">\(x = By\)</span>.
Depending on the structure of the problem, it may not be possible to design a unique mapping from <span class="math">\(A\)</span> to <span class="math">\(B\)</span>.</p>
<p>If <span class="math">\(A\)</span> is taller than it is wide, then it is possible for this equation to have no solution. If <span class="math">\(A\)</span> is wider than it is tall, then there could be multiple possible solutions.
The pseudoinverse of <span class="math">\(A\)</span> is defined as a&nbsp;matrix</p>
<div class="math">
\begin{equation*}
A^{+} = \lim_{\alpha\to 0} (A^TA + \alpha I)^{-1}A^T
\end{equation*}
</div>
<p>Practical algorithms for computing the pseudoinverse are not based on this definition, but rather the&nbsp;formula</p>
<div class="math">
\begin{equation*}
A^{+} = VD^{+}U^T
\end{equation*}
</div>
<p>where <span class="math">\(U\)</span>, <span class="math">\(D\)</span> and <span class="math">\(V\)</span> are the singular value decomposition of <span class="math">\(A\)</span>, and the pseudoinverse <span class="math">\(D^{+}\)</span> of a diagonal matrix <span class="math">\(D\)</span> is obtained by taking the reciprocal of its non-zero elements then taking the transpose of the resulting&nbsp;matrix.</p>
<p>When <span class="math">\(A\)</span> has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions. Specifically, it provides the solution <span class="math">\(x=A^{+}y\)</span> with minimal Euclidean norm <span class="math">\(||x||_2\)</span> among all possible&nbsp;solutions.</p>
<p>When <span class="math">\(A\)</span> has more rows than columns, it is possible for there to be no solution. In this case, using the pseudoinverse gives us the <span class="math">\(x\)</span> for which <span class="math">\(Ax\)</span> is as close as possible to <span class="math">\(y\)</span> in terms of Euclidean norm <span class="math">\(||Ax - y||_2\)</span>.</p>
</div>
<div class="section" id="the-trace-operator">
<h2><a class="toc-backref" href="#id8">The Trace&nbsp;Operator</a></h2>
<p>Trace operator gives the sum of all of the diagonal entries of a&nbsp;matrix:</p>
<div class="math">
\begin{equation*}
Tr(A) = \sum_i A_{i, i}
\end{equation*}
</div>
<p>For example, the trace operator provides an alternative way of writing the Frobenius norm of a&nbsp;matrix:</p>
<div class="math">
\begin{equation*}
||A||_F=\sqrt{(Tr(AA^T))}
\end{equation*}
</div>
<p>Also: <span class="math">\(Tr(A) = Tr(A^T)\)</span>, and <span class="math">\(Tr(ABC) = Tr(CAB) = Tr(BCA)\)</span>.</p>
</div>
<div class="section" id="the-determinant">
<h2><a class="toc-backref" href="#id9">The&nbsp;Determinant</a></h2>
<p>The determinant of a square matrix, denoted <span class="math">\(det(A)\)</span>, is a function mapping matrices to real scalars.
The determinant is equal to the product of all the eigenvalues of the matrix. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the transformation preserves&nbsp;volume.</p>
</div>
</div>
    </div>
    
</article><!--End of body content--><footer id="footer">
            Contents © 2019         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

        <script src="../../../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92406723-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>