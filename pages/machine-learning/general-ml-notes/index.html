<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<base href="https://ikhlestov.github.io/pages/machine-learning/general-ml-notes/">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>General ML Notes | Illarion Khlestov Blog</title>
<link href="../../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/pages/machine-learning/general-ml-notes/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'left', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="General ML Notes">
<meta property="og:url" content="https://ikhlestov.github.io/pages/machine-learning/general-ml-notes/">
<meta property="og:description" content="This notes based on Neural Networks and Deep Learning
and Coursera ML Courses. They may seems to be some way unstructured, but work still in progress, so please be patient.

Contents:

Ֆ General Appro">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-10-02T23:00:05Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../../">Blog</a>
                </li>
<li>
<a href="../../">Pages</a>
                </li>
<li>
<a href="../../../listings/">Listings</a>
                </li>
<li>
<a href="../../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="storypage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">General ML&nbsp;Notes</a></h1>

        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>This notes based on <a class="reference external" href="http://neuralnetworksanddeeplearning.com/index.html">Neural Networks and Deep Learning</a>
and <a class="reference external" href="https://www.coursera.org/learn/machine-learning">Coursera ML Courses</a>. They may seems to be some way unstructured, but work still in progress, so please be&nbsp;patient.</p>
<div class="contents topic" id="contents">
<p class="topic-title first">Contents:</p>
<ul class="simple">
<li><a class="reference internal" href="#general-approach" id="id1">Ֆ General&nbsp;Approach</a></li>
<li><a class="reference internal" href="#definitions" id="id2">Ֆ&nbsp;Definitions</a></li>
<li><a class="reference internal" href="#part-i" id="id3">Ֆ Part&nbsp;I</a></li>
<li><a class="reference internal" href="#part-ii" id="id4">Ֆ Part&nbsp;II</a></li>
<li>
<a class="reference internal" href="#part-iii" id="id5">Ֆ Part III</a><ul>
<li><a class="reference internal" href="#cost-functions" id="id6">Cost&nbsp;functions</a></li>
<li>
<a class="reference internal" href="#how-to-choose-a-neural-network-s-hyper-parameters" id="id7">How to choose a neural network&#8217;s hyper-parameters?</a><ul>
<li><a class="reference internal" href="#broad-strategy" id="id8">Broad&nbsp;strategy</a></li>
<li><a class="reference internal" href="#learning-rate" id="id9">Learning rate (<strong>η</strong>)</a></li>
<li><a class="reference internal" href="#using-early-stopping" id="id10">Using early&nbsp;stopping</a></li>
<li><a class="reference internal" href="#learning-rate-schedule" id="id11">Learning rate&nbsp;schedule</a></li>
<li><a class="reference internal" href="#mini-batch-size" id="id12">Mini-batch&nbsp;size</a></li>
<li><a class="reference internal" href="#automated-techniques" id="id13">Automated&nbsp;techniques</a></li>
<li><a class="reference internal" href="#futher-reading" id="id14">Futher&nbsp;reading</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#evaluation-of-algorithm" id="id15">Ֆ Evaluation of&nbsp;algorithm</a></li>
<li><a class="reference internal" href="#overfiting-and-underfitting" id="id16">Ֆ Overfiting and&nbsp;underfitting</a></li>
</ul>
</div>
<div class="section" id="general-approach">
<h2><a class="toc-backref" href="#id1">Ֆ General&nbsp;Approach</a></h2>
<ul class="simple">
<li>Define network&nbsp;architecture</li>
<li>Choose right cost&nbsp;function</li>
<li>Calculate gradient descent if&nbsp;necessary</li>
<li>Train, tune&nbsp;hyperparameters.</li>
</ul>
</div>
<div class="section" id="definitions">
<h2><a class="toc-backref" href="#id2">Ֆ&nbsp;Definitions</a></h2>
<ul class="simple">
<li>
<strong>One-shot learning</strong> - aim to lean not from thousands of examples but from one or only a&nbsp;few.</li>
<li>
<strong>Transfer learning</strong> - apply already trained model with previous knowledge to the new&nbsp;domain.</li>
</ul>
</div>
<div class="section" id="part-i">
<h2><a class="toc-backref" href="#id3">Ֆ Part&nbsp;I</a></h2>
<p>Sigmoid&nbsp;function:</p>
<div class="math">
\begin{equation*}
\sigma(z) = \frac{1}{1 + e^{-z}}
\end{equation*}
</div>
<p><span class="math">\(\sigma(\infty)\approx 1\)</span>, <span class="math">\(\sigma(-\infty)\approx 0\)</span>,
but note, that <span class="math">\(\sigma(0)=1\)</span></p>
<p>Note: <em>sigmoid function</em> (<span class="math">\(\sigma\)</span>) == <em>logistic function</em>
so <em>sigmoid neurons</em> can be called as <em>logistic neurons</em>.</p>
<p><strong>MLP</strong> is an abbreviation for <em>multilayer&nbsp;perceptrons</em></p>
<p><em>cost</em> fucntion == <em>loss</em> function == <em>objective</em>&nbsp;function.</p>
<p><em>Quadratic cost function</em> (or <em>mean squared error</em>, or just <em>MSE</em>):</p>
<div class="math">
\begin{equation*}
C(w,b)  = \frac{1}{2n}\sum_{n}||y(x) - a||^2
\end{equation*}
</div>
<p>Here,
<em>w</em> denotes the collection of all weights in the network,
<em>b</em> all the biases,
<em>n</em> is the total number of training inputs,
<em>a</em> is the vector of outputs from the network when <em>x</em> is input,
and the sum is over all training inputs, <em>x</em>.</p>
<p>An idea of <em>stochastic gradient descent</em> is to estimate the gradient
<span class="math">\(\nabla C\)</span> by computing <span class="math">\(\nabla Cx\)</span> for a small sample of randomly chosen training inputs,
not for all inputs as usual <em>gradient descent</em> do.
For this stochastic gradient descent take small number of <em>m</em> randomly chosen training inputs.
We&#8217;ll label those random training inputs <span class="math">\(X1,X2,\ldots  ,Xm\)</span> and refer to them as a <em>mini-batch</em>.
So now gradinet can be computed&nbsp;as:</p>
<div class="math">
\begin{equation*}
\nabla C \approx \frac{1}{m}\sum_{j=1}^m \nabla C_{X_j}
\end{equation*}
</div>
</div>
<div class="section" id="part-ii">
<h2><a class="toc-backref" href="#id4">Ֆ Part&nbsp;II</a></h2>
<img alt="/images/ML_notes/weights_notation.png" src="../../../images/ML_notes/weights_notation.png"><p>image from <a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap2.html">this&nbsp;book</a></p>
<p><em>Elementwise</em> product of the two vectors denoted as <span class="math">\(s \odot t\)</span> and can be called sometimes <em>Hadamard product</em> or <em>Schur product</em>.
Her is an&nbsp;example:</p>
<div class="math">
\begin{equation*}
\left[\begin{array}{c} 1 &#92;\ 2 \end{array}\right]
  \odot \left[\begin{array}{c} 3 &#92;\ 4\end{array} \right]
= \left[ \begin{array}{c} 1 * 3 &#92;\ 2 * 4 \end{array} \right]
= \left[ \begin{array}{c} 3 &#92;\ 8 \end{array} \right]
\end{equation*}
</div>
<p>In tensorflow you should distinguish usual matrix multiplication and hadamard&nbsp;product</p>
<pre class="code python"><a name="rest_code_046cff47860c4f80aa17662c862bf702-1"></a><span class="c1"># W, Q - some matrices</span>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-2"></a>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-3"></a><span class="c1"># matrix multiplication</span>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-4"></a><span class="n">res</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-5"></a>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-6"></a><span class="c1"># hadamard product</span>
<a name="rest_code_046cff47860c4f80aa17662c862bf702-7"></a><span class="n">res</span> <span class="o">=</span> <span class="n">W</span> <span class="o">*</span> <span class="n">Q</span>
</pre>
</div>
<div class="section" id="part-iii">
<h2><a class="toc-backref" href="#id5">Ֆ Part&nbsp;III</a></h2>
<div class="section" id="cost-functions">
<h3><a class="toc-backref" href="#id6">Cost&nbsp;functions</a></h3>
<p>Note that cost function should be non&nbsp;negative!</p>
<p><strong>Quadratic cost function</strong> <span class="math">\(C = \frac{(y-a)^2}{2}\)</span>.</p>
<p><strong>Cross-entropy cost function</strong> <span class="math">\(C = -\frac{1}{n} \sum_x \left[y \ln a + (1-y ) \ln (1-a)&nbsp;\right]\)</span></p>
<p>Where:</p>
<ul class="simple">
<li>
<span class="math">\(n\)</span> - the total number of items of training&nbsp;data</li>
<li>
<span class="math">\(x\)</span> - the sum is over all training&nbsp;inputs</li>
<li>
<span class="math">\(y\)</span> - corresponding desired&nbsp;output</li>
</ul>
</div>
<div class="section" id="how-to-choose-a-neural-network-s-hyper-parameters">
<h3><a class="toc-backref" href="#id7">How to choose a neural network&#8217;s&nbsp;hyper-parameters?</a></h3>
<div class="section" id="broad-strategy">
<h4><a class="toc-backref" href="#id8">Broad&nbsp;strategy</a></h4>
<ul class="simple">
<li>Simplify the&nbsp;model</li>
<li>Reduce classification&nbsp;classes</li>
<li>Reduce training/validation&nbsp;data</li>
<li>Increase frequency of&nbsp;monitoring</li>
<li>With such updates you may try to find required hyper-parameters very&nbsp;fast</li>
</ul>
</div>
<div class="section" id="learning-rate">
<h4><a class="toc-backref" href="#id9">Learning rate (<strong>η</strong>)</a></h4>
<ul class="simple">
<li>Estimate the threshold value for <strong>η</strong> at which the cost on the training data immediately begins decreasing, instead of oscillating or&nbsp;increasing.</li>
<li>After you likely want to use value of <strong>η</strong> that is smaller, say, a factor of two bellow the&nbsp;threshold.</li>
</ul>
</div>
<div class="section" id="using-early-stopping">
<h4><a class="toc-backref" href="#id10">Using early&nbsp;stopping</a></h4>
<p>A better rule is to terminate if the best classification accuracy doesn&#8217;t improve for quite some time.
For example we might elect to terminate if the classification accuracy hasn&#8217;t improved during the last ten&nbsp;epochs.</p>
</div>
<div class="section" id="learning-rate-schedule">
<h4><a class="toc-backref" href="#id11">Learning rate&nbsp;schedule</a></h4>
<p>We need choose when learning rate should be decreased and by what rule. Some of existing rules&nbsp;are:</p>
<ul class="simple">
<li>
<strong>Step decay</strong> - reduce learning rate by some&nbsp;factor.</li>
<li>
<strong>Exponental decay</strong> - <span class="math">\(\alpha = \alpha_0 e^{-k t}\)</span>, where <span class="math">\(\alpha_0, k\)</span> are hyperparameters and <span class="math">\(t\)</span> is the iteration number (but you can also use units of&nbsp;epochs).</li>
<li>
<strong>1/t decay</strong> - <span class="math">\(\alpha = \alpha_0 / (1 + k t )\)</span>, where <span class="math">\(\alpha_0, k\)</span> are hyperparameters and <span class="math">\(t\)</span> is the iteration&nbsp;number.</li>
</ul>
<p>Also you may checked <a class="reference external" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/learning_rate_decay.py">predefined learning schedules at tensorflow</a>.
But prior to use learning rate schedule it&#8217;s better to get best performed model with fixed learning&nbsp;rate.</p>
</div>
<div class="section" id="mini-batch-size">
<h4><a class="toc-backref" href="#id12">Mini-batch&nbsp;size</a></h4>
<p>Wights updates for online learning can be declarated&nbsp;as:</p>
<div class="math">
\begin{equation*}
w \rightarrow w&#8217; = w-\eta \nabla C_x
\end{equation*}
</div>
<p>For case of mini-batch of size 100 we&nbsp;get:</p>
<div class="math">
\begin{equation*}
w \rightarrow w&#8217; = w-\eta \frac{1}{100} \sum_x \nabla C_x
\end{equation*}
</div>
<p>With this we may increase learning rate by a factor 100 and updated rules&nbsp;become:</p>
<div class="math">
\begin{equation*}
w \rightarrow w&#8217; = w-\eta \sum_x \nabla C_x
\end{equation*}
</div>
<p>With choosing mini-batch size we shouldn&#8217;t update any others hyper-parameters, only learning rate should be checked. After we may try different mini-batches sizes, scaling learning rate as required and choose what validation accuracy updates faster at real time(not related to epochs) in order to maximize our model overall&nbsp;speed.</p>
</div>
<div class="section" id="automated-techniques">
<h4><a class="toc-backref" href="#id13">Automated&nbsp;techniques</a></h4>
<p>For automated hyper-parameters choose we can use
<a class="reference external" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">grid search</a>
or something like
<a class="reference external" href="http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf">Bayesian approach</a>
(<a class="reference external" href="https://github.com/jaberg/hyperopt">source code</a>)</p>
</div>
<div class="section" id="futher-reading">
<h4><a class="toc-backref" href="#id14">Futher&nbsp;reading</a></h4>
<ul class="simple">
<li><a class="reference external" href="https://arxiv.org/pdf/1206.5533v2.pdf">Practical recommendations for gradient-based training of deep&nbsp;architectures</a></li>
<li><a class="reference external" href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient&nbsp;BackProp</a></li>
<li>
<a class="reference external" href="http://www.springer.com/gp/book/9783642352881">Neural Networks: Tricks of the Trade</a> (you may try not to use hwole book, but search for some articles from its&nbsp;authors)</li>
</ul>
</div>
</div>
</div>
<div class="section" id="evaluation-of-algorithm">
<h2><a class="toc-backref" href="#id15">Ֆ Evaluation of&nbsp;algorithm</a></h2>
<p>What we should&nbsp;do:</p>
<ol class="arabic simple">
<li>Split the dataset into three portions: train set, validate set and test set, in a proportion&nbsp;3:1:1.</li>
<li>When the number of examples <em>m</em> increase, the cost <span class="math">\({J_{test}}\)</span> increases, while <span class="math">\({J_{val}}\)</span> decrease. When <em>m</em> is very large, if <span class="math">\({J_{test}}\)</span> is about equal to <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large bias(underfiting), while if there is a gap between <span class="math">\({J_{test}}\)</span> and <span class="math">\({J_{val}}\)</span> the algorithm may suffer from large&nbsp;variance(overfitting).</li>
<li>To solve the problem of large bias, you may decrease <span class="math">\({\rm{\lambda }}\)</span> in regularization, while increase it for the problem of large&nbsp;variance.</li>
<li>To evaluate the performance of a classification algorithm, we can use the value: precision, recall and&nbsp;F1.</li>
</ol>
<p>Precision:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalsePositive}}
\end{equation*}
</div>
<p>Recall:</p>
<div class="math">
\begin{equation*}
\frac{{TruePositive}}{{TruePositive + FalseNegtive}}
\end{equation*}
</div>
<p>F1:</p>
<div class="math">
\begin{equation*}
\frac{{2*Recall*Precision}}{{Recall + Precision}}
\end{equation*}
</div>
</div>
<div class="section" id="overfiting-and-underfitting">
<h2><a class="toc-backref" href="#id16">Ֆ Overfiting and&nbsp;underfitting</a></h2>
<p>High <strong>bias</strong> is <strong>underfitting</strong> and high <strong>variance</strong> is <strong>overfitting</strong>.</p>
<p>For understanding what exactly mean <em>Bias</em> and <em>Variance</em> you may check <a class="reference external" href="http://scott.fortmann-roe.com/docs/BiasVariance.html">this</a>
or <a class="reference external" href="http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/">this</a>
cool&nbsp;articles.</p>
<p>Next notes based on awesome Andre Ng <a class="reference external" href="https://www.youtube.com/watch?v=F1ka6a13S9I">lecture</a></p>
<p>During training as usual you split your data on train, validation and test sets.
<em>Note:</em> You should keep your validation/test data the same for model you want to compare.
After measuring errors you can get some results.
In this case difference between <em>human error</em> (how human perform such task) and <em>train error</em> will be <strong>bias</strong>.
On the other hand, difference between <em>train error</em> and <em>validation error</em> will be <strong>variance</strong>.</p>
<object data="../../../images/ML_notes/bias_variance_explanation_1.svg" style="width: 320px; height: 120px;" type="image/svg+xml">
bias_variance_explanation_1</object>
<p>In such case you should consider this&nbsp;methods</p>
<object data="../../../images/ML_notes/bias_variance_workflow_1.svg" style="width: 443px; height: 402px;" type="image/svg+xml">
bias_variance_workflow_1</object>
<p>Solutions inside blue boxes should be applied as first&nbsp;approach.</p>
<p>But sometimes you may have a lot of data from one domain, but test data comes from another.
In this case validation and test data should be from the same domain.
Also you may consider get validation data also from large domain.
But it should be additional validation(say <em>train-valid</em>).
Let&#8217;s see an&nbsp;example.</p>
<object data="../../../images/ML_notes/data_spliting_in_domains.svg" style="width: 473px; height: 93px;" type="image/svg+xml">
data_spliting_in_domains</object>
<p>In this case we receive another correlation between&nbsp;errors:</p>
<object data="../../../images/ML_notes/bias_variance_explanation_2.svg" style="width: 453px; height: 166px;" type="image/svg+xml">
bias_variance_explanation_2</object>
<p>And solution algorithm will be a little bit more&nbsp;longer:</p>
<object data="../../../images/ML_notes/bias_variance_workflow_2.svg" style="width: 443px; height: 675px;" type="image/svg+xml">
bias_variance_workflow_2</object>
</div>
</div>
    </div>
    
</article>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents © 2016         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>