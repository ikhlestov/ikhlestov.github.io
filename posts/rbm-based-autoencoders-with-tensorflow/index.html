<!DOCTYPE html>
<html prefix="
og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>RBM based Autoencoders with tensorflow | Illarion&#8217;s Notes</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/">
<link rel="icon" href="../../favicon.ico" sizes="16x16">
<link rel="icon" href="../../favicon.png" sizes="128x128">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'center', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta property="og:site_name" content="Illarion's Notes">
<meta property="og:title" content="RBM based Autoencoders with tensorflow">
<meta property="og:url" content="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/">
<meta property="og:description" content="Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in Semantic Hashing paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pr">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-12-28T20:33:15Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="https://ikhlestov.github.io/">

            <span id="blog-title">Illarion&#8217;s Notes</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item">
<a href="../../" class="nav-link">About</a>
                </li>
<li class="nav-item">
<a href="../" class="nav-link">Blog&nbsp;Posts</a>
                </li>
<li class="nav-item">
<a href="../../pages/" class="nav-link">Pages</a>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <nav class="breadcrumbs"><ul class="breadcrumb">
<li class="breadcrumb-item"><a href="../">posts</a></li>
                <li class="breadcrumb-item active">rbm-based-autoencoders-with-tensorflow</li>
</ul></nav><div class="body-content">
        <!--Body content-->
        
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">RBM based Autoencoders with&nbsp;tensorflow</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                    Illarion&nbsp;Khlestov
            </span></p>
            <p class="dateline">
            <a href="." rel="bookmark">
            <time class="published dt-published" datetime="2016-12-28T20:33:15Z" itemprop="datePublished" title="2016-12-28 20:33">2016-12-28 20:33</time></a>
            </p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/posts/rbm-based-autoencoders-with-tensorflow.html">Comments</a>


            
        </p>
</div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p>Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in <a class="reference external" href="http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf">Semantic Hashing</a> paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I&#8217;ve decided to check&nbsp;this.</p>
<!-- TEASER_END -->
<p>This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader&#8217;s previous knowledge of tensorflow and machine learning field. All code can be found in <a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow">this&nbsp;repo</a></p>
<p>RBMs different from usual neural networks in some&nbsp;ways:</p>
<p>Neural networks usually perform weight update by Gradient Descent, but RMBs use <strong>Contrastive Divergence</strong> (which is basically a funky term for &#8220;approximate gradient descent&#8221; <a class="reference external" href="http://deeplearning.net/tutorial/rbm.html">link to read</a>). At a glance, contrastive divergence computes a difference between <strong>positive phase</strong> (energy of first encoding) and <strong>negative phase</strong> (energy of the last&nbsp;encoding).</p>
<pre class="code python"><a name="rest_code_970e4d7822ee4c1188d12119865a375d-1"></a><span class="n">positive_phase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
<a name="rest_code_970e4d7822ee4c1188d12119865a375d-2"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">visib_inputs_initial</span><span class="p">),</span> <span class="n">first_encoded_probability</span><span class="p">)</span>
<a name="rest_code_970e4d7822ee4c1188d12119865a375d-3"></a><span class="n">negative_phase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
<a name="rest_code_970e4d7822ee4c1188d12119865a375d-4"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">last_reconstructed_probability</span><span class="p">),</span> <span class="n">last_encoded_probability</span><span class="p">)</span>
<a name="rest_code_970e4d7822ee4c1188d12119865a375d-5"></a><span class="n">contrastive_divergence</span> <span class="o">=</span> <span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span>
</pre>
<p>Also, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read <a class="reference external" href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/">here</a> or <a class="reference external" href="http://rocknrollnerd.github.io/ml/2015/07/18/general-boltzmann-machines.html">here</a>.</p>
<p>As prototype one layer tensorflow rbm <a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/models/rbm_models/rbm.py">implementation</a> was used. For testing, I&#8217;ve taken well known <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset(dataset of handwritten&nbsp;digits).</p>
<div class="section" id="many-layers-implementation">
<h2>Many layers&nbsp;implementation</h2>
<p>At first, I&#8217;ve implement <a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/rbm_all_layers_at_once.py">multilayers RBM</a> with three layers. Because we do not use usual tensorflow optimizers we may stop gradient for every variable with <cite>tf.stop_gradient(variable_name)</cite> and this will speed up computation a little bit. After construction two questions&nbsp;arose:</p>
<ul class="simple">
<li>Should every layer hidden units be binary encoded or only last&nbsp;one?</li>
<li>Should we update every layer weights/biases at once per step, or first train only first two layers, after layers 2 and 3, and so&nbsp;on?</li>
</ul>
<p>So I&#8217;ve run the model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while, I note that this approach was already proposed in the paper, but I somehow miss&nbsp;this.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png"></a>
<p class="caption">Errors with respect to&nbsp;steps</p>
</div>
<p>So let&#8217;s stop with the last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pre-trained first two layers weights/biases and continue train last two layers (<a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/rbm_train_by_pair_layers.py">code</a>). During implementation I&#8217;ve met some trouble - tensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn&#8217;t find this. So I&#8217;ve finished with approach when I directly send variable that should be restored and variables that should be&nbsp;initialized.</p>
<pre class="code python"><a name="rest_code_62df9eb2c874419db64d4589ff9d271a-1"></a><span class="c1"># restore previous variables</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-2"></a><span class="n">restore_vars_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_restored_variables_names</span><span class="p">()</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-3"></a><span class="n">restorer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">restore_vars_dict</span><span class="p">)</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-4"></a><span class="n">restorer</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">saves_path</span><span class="p">)</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-5"></a>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-6"></a><span class="c1"># initialize not restored variables</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-7"></a><span class="n">new_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_new_variables_names</span><span class="p">()</span>
<a name="rest_code_62df9eb2c874419db64d4589ff9d271a-8"></a><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_variables</span><span class="p">(</span><span class="n">new_variables</span><span class="p">))</span>
</pre>
<p>After testing seems that both training approaches converge to approximately same error. But some another cool stuff - the model that was trained by pair lairs trains faster in&nbsp;time.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png"></a>
<p class="caption">Errors with respect to&nbsp;steps</p>
</div>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png"></a>
<p class="caption">Errors with respect to time&nbsp;consumption</p>
</div>
<p>So we stop with RBM trained with only last layer binarized and with <em>two layers only</em>&nbsp;strategy.</p>
</div>
<div class="section" id="build-autoencoder-from-rbm">
<h2>Build autoencoder from&nbsp;RBM</h2>
<p>After getting pre-trained weights from RMB, it&#8217;s time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advice we add Gaussian noise before layer. To simulate <em>deterministic noise</em> behavior, noise generated for each input prior training and not changed during training. Also, we want compare autoencoder loaded from RBM weights with self-initialized usual autoencoder. <a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/autoencoder.py">Code for&nbsp;autoencoder</a></p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png"></a>
<p class="caption">RBM initialized autoencoder vs. newly initialized&nbsp;autoencoder</p>
</div>
<p>It seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some&nbsp;point.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png"></a>
<p class="caption">Only RBM based autoencoder training process, for&nbsp;clarity</p>
</div>
<p>Also, I&#8217;ve trained two autoencoders without Gaussian noise. Now we can see through distribution what embedding most similar to binary (<a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/visualize_distribution.py">code for visualization</a>):</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/04_rbm_aec_embeddings_distribution.thumbnail.png"></a>
<p class="caption">Comparison of embeddings&nbsp;distributions</p>
</div>
<p>We can see that RBM based autoencoder with Gaussian noise works better than other for our&nbsp;purposes.</p>
</div>
<div class="section" id="validation">
<h2>Validation</h2>
<p>To validate received embeddings I generate them for test and train sets for such&nbsp;networks:</p>
<ul class="simple">
<li>Initial MNIST(without embedding at&nbsp;all)</li>
<li>RBM with the last layer binarized and trained by&nbsp;pairs</li>
<li>Autoencoder based on RBM with Gaussian&nbsp;noise</li>
<li>Newly initialized autoencoder with Gaussian&nbsp;noise</li>
</ul>
<p>and use two validation&nbsp;approaches:</p>
<p>Train SVM with the train set and measure accuracy on the test set. SVM was used from sklearn with &#8216;rbf&#8217; kernel with no <cite>max_iter</cite> == 50. Results table were generated with <a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/svm_clusterization_test.py">this&nbsp;code</a></p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="56%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">notes</th>
<th class="head">accuracy</th>
<th class="head">prec</th>
<th class="head">f_score</th>
<th class="head">recall</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>default mnist dataset</td>
<td>0.446</td>
<td>0.647</td>
<td>0.460</td>
<td>0.454</td>
</tr>
<tr>
<td>rbm: train_layers_by_pairs  last_layer_binarized</td>
<td>0.455</td>
<td>0.450</td>
<td>0.446</td>
<td>0.453</td>
</tr>
<tr>
<td>autoencoder: rbm_initialized_model  with_Gaussian_noise</td>
<td>0.499</td>
<td>0.500</td>
<td>0.493</td>
<td>0.494</td>
</tr>
<tr>
<td>autoencoder: new_initialized_model  with_Gaussian_noise</td>
<td>0.100</td>
<td>0.098</td>
<td>0.095</td>
<td>0.099</td>
</tr>
</tbody>
</table>
<p>With Hamming distance or dot product find ten most similar pictures/embeddings to provided one and check how many labels are the same to the submitted array label. <a class="reference external" href="https://github.com/ikhlestov/rbm_based_autoencoders_with_tensorflow/blob/master/results_validation/found_similiar.py">Code</a> to check distance&nbsp;accuracies.</p>
<table border="1" class="colwidths-given docutils">
<colgroup>
<col width="56%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead valign="bottom"><tr>
<th class="head">notes</th>
<th class="head">hamming accuracy</th>
<th class="head">hamming time_cons</th>
<th class="head">dot_product accuracy</th>
<th class="head">dot_product time_cons</th>
</tr></thead>
<tbody valign="top">
<tr>
<td>default mnist dataset</td>
<td>0.910</td>
<td>180.4</td>
<td>0.916</td>
<td>528.8</td>
</tr>
<tr>
<td>rbm: train_layers_by_pairs  last_layer_binarized</td>
<td>0.633</td>
<td>28.6</td>
<td>0.638</td>
<td>60.2</td>
</tr>
<tr>
<td>autoencoder: rbm_initialized_model  with_Gaussian_noise</td>
<td>0.583</td>
<td>28.9</td>
<td>0.563</td>
<td>61.6</td>
</tr>
<tr>
<td>autoencoder: new_initialized_model  with_Gaussian_noise</td>
<td>0.099</td>
<td>29.8</td>
<td>0.099</td>
<td>64.6</td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="conclusion">
<h2>Conclusion</h2>
<p>As we can see embeddings can save some strong features, that can be used for future clusterization very well. But these features are not linearly correlated - so when we measure accuracy for most similar embeddings, we get results worse than when we use full MNIST images. Of course, maybe autoencoder should be trained with another learning rate/longer, but this is the task for future&nbsp;research.</p>
<p>At the same time, we confirmed that training autoencoders from pre-trained RBMs weights are right to approach - the network will pass local optimization minimum and not stack at some point during&nbsp;training.</p>
</div>
<div class="section" id="training-params">
<h2>Training&nbsp;params</h2>
<p>For RBM training such params were used network was trained&nbsp;with:</p>
<ul class="simple">
<li>epochs =&nbsp;6</li>
<li>learning rate =&nbsp;0.01</li>
<li>batch size =&nbsp;100</li>
<li>shuffle batches =&nbsp;True</li>
<li>gibbs sampling steps =&nbsp;1</li>
<li>layers quantity =&nbsp;3</li>
<li>layers shapes(including input layer) = [784, 484, 196,&nbsp;100]</li>
</ul>
<p>For autoencoder learning rate was changed to 1.0 because of another optimization&nbsp;rule.</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ikhlestov-blog",
            disqus_url="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/",
        disqus_title="RBM based Autoencoders with tensorflow",
        disqus_identifier="cache/posts/rbm-based-autoencoders-with-tensorflow.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="ikhlestov-blog";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script><!--End of body content--><footer id="footer">
            Contents © 2019         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

        <script src="../../assets/js/all-nocdn.js"></script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates --><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element) {
            return element.getElementsByTagName('img')[0].alt;
    }});
    </script><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-92406723-1', 'auto');
  ga('send', 'pageview');

</script>
</body>
</html>