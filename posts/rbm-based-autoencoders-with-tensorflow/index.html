<html prefix="
og: http://ogp.me/ns#
article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<base href="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>RBM based Autoencoders with tensorflow | Illarion Khlestov Blog</title>
<link href="../../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta content="#5670d4" name="theme-color">
<link rel="alternate" type="application/rss+xml" title="RSS" href="../../rss.xml">
<link rel="canonical" href="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/">
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    displayAlign: 'left', // Change this to 'center' to center equations.
});
</script><!--[if lt IE 9]><script src="../../assets/js/html5.js"></script><![endif]--><meta name="author" content="Illarion Khlestov">
<meta name="robots" content="noindex">
<meta property="og:site_name" content="Illarion Khlestov Blog">
<meta property="og:title" content="RBM based Autoencoders with tensorflow">
<meta property="og:url" content="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/">
<meta property="og:description" content="This is draft of post - feel free to comment/mail me about any errors
Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in Semantic Hashing paper by Ruslan Sala">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2016-12-28T20:33:15Z">
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-inverse navbar-static-top"><div class="container">
<!-- This keeps the margins nice -->
        <div class="navbar-header">
            <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="https://ikhlestov.github.io/">

                <span id="blog-title">Illarion Khlestov Blog</span>
            </a>
        </div>
<!-- /.navbar-header -->
        <div class="collapse navbar-collapse" id="bs-navbar" aria-expanded="false">
            <ul class="nav navbar-nav">
<li>
<a href="../../">Blog</a>
                </li>
<li>
<a href="../../pages/">Pages</a>
                </li>
<li>
<a href="../../listings/">Listings</a>
                </li>
<li>
<a href="../../archive.html">Archive</a>

                
            </li>
</ul>
<ul class="nav navbar-nav navbar-right">
<li>
    <a href="index.rst" id="sourcelink">Source</a>
    </li>
                
            </ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        <div class="row">
            
<article class="post-text h-entry hentry postpage" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title" itemprop="headline name"><a href="." class="u-url">RBM based Autoencoders with&nbsp;tensorflow</a></h1>

        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn">
                    Illarion&nbsp;Khlestov
            </span></p>
            <p class="dateline"><a href="." rel="bookmark"><time class="published dt-published" datetime="2016-12-28T20:33:15Z" itemprop="datePublished" title="2016-12-28 20:33">2016-12-28 20:33</time></a></p>
                <p class="commentline">            <a href="#disqus_thread" data-disqus-identifier="cache/posts/rbm-based-autoencoders-with-tensorflow.html">Comments</a>


                    </p>
<p class="sourceline"><a href="index.rst" id="sourcelink">Source</a></p>

        </div>
        
    </header><div class="e-content entry-content" itemprop="articleBody text">
    <div>
<p><strong>This is draft of post - feel free to comment/mail me about any&nbsp;errors</strong></p>
<p>Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in <a class="reference external" href="http://www.cs.utoronto.ca/~rsalakhu/papers/semantic_final.pdf">Semantic Hashing</a> paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with RBM pretrained weights autoencoders should converge faster. So I&#8217;ve decide to check&nbsp;this.</p>
<p>This post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader previous knowledge of tensorflow and machine learning&nbsp;field.</p>
<p>RBMs different from usual neural networks in some&nbsp;ways:</p>
<p>Neural networks usually perform weight update by Gradient Descent, but RMBs use <strong>Contrastive Divergence</strong> (which is basically a funky term for &#8220;approximate gradient descent&#8221; <a class="reference external" href="http://deeplearning.net/tutorial/rbm.html">link to read</a>). At a glance contrastive divergence compute difference between <strong>positive phase</strong> (energy of first encoding) and <strong>negative phase</strong> (energy of last&nbsp;encoding).</p>
<pre class="code python"><a name="rest_code_5fa6755c9c3d4a74ae75d52633982e88-1"></a><span class="n">positive_phase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
<a name="rest_code_5fa6755c9c3d4a74ae75d52633982e88-2"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">visib_inputs_initial</span><span class="p">),</span> <span class="n">first_encoded_probability</span><span class="p">)</span>
<a name="rest_code_5fa6755c9c3d4a74ae75d52633982e88-3"></a><span class="n">negative_phase</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
<a name="rest_code_5fa6755c9c3d4a74ae75d52633982e88-4"></a>    <span class="n">tf</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">last_reconstructed_probability</span><span class="p">),</span> <span class="n">last_encoded_probability</span><span class="p">)</span>
<a name="rest_code_5fa6755c9c3d4a74ae75d52633982e88-5"></a><span class="n">contrastive_divergence</span> <span class="o">=</span> <span class="n">positive_phase</span> <span class="o">-</span> <span class="n">negative_phase</span>
</pre>
<p>Also key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read <a class="reference external" href="http://blog.echen.me/2011/07/18/introduction-to-restricted-boltzmann-machines/">here</a> or <a class="reference external" href="http://rocknrollnerd.github.io/ml/2015/07/18/general-boltzmann-machines.html">here</a>.</p>
<p>As prototype one layer tensorflow rbm <a class="reference external" href="https://github.com/blackecho/Deep-Learning-TensorFlow/blob/master/yadlt/models/rbm_models/rbm.py">implementation</a> was used. For testing I&#8217;ve take usual <a class="reference external" href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset(dataset of handwritten&nbsp;digits).</p>
<div class="section" id="many-layers-implementation">
<h2>Many layers&nbsp;implementation</h2>
<p>At first I&#8217;ve implement <a class="reference external" href="fill_link">multi layers RBM(Add link to code here)</a> with 3 layers. Because we not use usual tensorflow optimizers we may stop gradient for every variable with <cite>tf.stop_gradient(variable_name)</cite> and this will speed up computation a little bit. After construction two questions arised:
- Should every layer hidden units be binary encoded or only last one?
- Should we update every layer weights/biases at once per step, or first train only first two layers, after layers 2 and 3, and so&nbsp;on?</p>
<p>So I&#8217;ve run model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while I note that this approach was already proposed in the paper, but I somehow miss&nbsp;this.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/01_layers_binarization.thumbnail.png"></a>
<p class="caption">Errors with respect to&nbsp;steps</p>
</div>
<p>So let&#8217;s stop with last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pretrained first two layers weights/biases and continue train last two layers. During implementation I&#8217;ve meet some trouble - tensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn&#8217;t find this. So I&#8217;ve finish with approach when I directly send variable that should be restored and variables that should be&nbsp;initialized.</p>
<pre class="code python"><a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-1"></a><span class="c1"># restore previous variables</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-2"></a><span class="n">restore_vars_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_restored_variables_names</span><span class="p">()</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-3"></a><span class="n">restorer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">(</span><span class="n">restore_vars_dict</span><span class="p">)</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-4"></a><span class="n">restorer</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">saves_path</span><span class="p">)</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-5"></a>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-6"></a><span class="c1"># initialize not restored variables</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-7"></a><span class="n">new_variables</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_new_variables_names</span><span class="p">()</span>
<a name="rest_code_34d2cc022909488b9c5b96e5da4089ef-8"></a><span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">initialize_variables</span><span class="p">(</span><span class="n">new_variables</span><span class="p">))</span>
</pre>
<p>After testing is seems that both training approaches converge to approximately same error. But some another cool stuff - model that was trained by pair lairs trains faster in&nbsp;time.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_error.thumbnail.png"></a>
<p class="caption">Errors with respect to&nbsp;steps</p>
</div>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/02_layers_training_relative.thumbnail.png"></a>
<p class="caption">Errors with respect to time&nbsp;consumption</p>
</div>
<p>So we stop with RBM trained with only last layer binarized and trained with <em>two layers only</em>&nbsp;strategy.</p>
</div>
<div class="section" id="build-autoencoder-from-rbm">
<h2>Build autoencoder from&nbsp;RBM</h2>
<p>After get pretrained weights from RMB it&#8217;s time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advise we add Gaussian noise prior to layer. To simulate <em>deterministic noise</em> behavior, noise was generate for each input prior training and not changed during training. Also we want compare autoencoder loaded from RBM weights with self initialized usual&nbsp;autoencoder.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_and_new_initialized_autoencoders.thumbnail.png"></a>
<p class="caption">RBM initialized autoencoder vs newly initialized&nbsp;autoencoder</p>
</div>
<p>It seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some&nbsp;point.</p>
<div class="figure">
<a class="reference external image-reference" href="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.png"><img alt="/images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png" src="../../images/rbm-based-autoencoders-with-tensorflow/03_rbm_initialized_autoencoder.thumbnail.png"></a>
<p class="caption">Only RBM based autoencoder training process, for&nbsp;clarity</p>
</div>
<p>TODO: Visualize distribution of probabilities that should be converted&nbsp;to</p>
</div>
<div class="section" id="validation">
<h2>Validation</h2>
<p>To validate received embeddings I generate them for test and train sets and use two approaches:
- Train SVM with train set and measure accuracy on test set.
- With hamming distance or dot product find 10 most similar pictures/embeddings to provided one and check how many labels are the same to provided array&nbsp;label.</p>
<p>TODO:
Final testing of all embeddings variants
links to models
links to validation approaches
description of&nbsp;params</p>
</div>
</div>
    </div>
    <aside class="postpromonav"><nav></nav></aside><section class="comments hidden-print"><h2>Comments</h2>
                        <div id="disqus_thread"></div>
        <script>
        var disqus_shortname ="ikhlestov-blog",
            disqus_url="https://ikhlestov.github.io/posts/rbm-based-autoencoders-with-tensorflow/",
        disqus_title="RBM based Autoencoders with tensorflow",
        disqus_identifier="cache/posts/rbm-based-autoencoders-with-tensorflow.html",
        disqus_config = function () {
            this.language = "en";
        };
        (function() {
            var dsq = document.createElement('script'); dsq.async = true;
            dsq.src = 'https://' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
    <a href="https://disqus.com" class="dsq-brlink" rel="nofollow">Comments powered by <span class="logo-disqus">Disqus</span></a>


        </section></article><script>var disqus_shortname="ikhlestov-blog";(function(){var a=document.createElement("script");a.async=true;a.src="https://"+disqus_shortname+".disqus.com/count.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(a)}());</script>
</div>
        <!--End of body content-->

        <footer id="footer">
            Contents Â© 2016         <a href="mailto:ikhlestov@gmail.com">Illarion Khlestov</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>          <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"> </script></footer>
</div>
</div>

            <script src="../../assets/js/all-nocdn.js"></script><script>$('a.image-reference:not(.islink) img:not(.islink)').parent().colorbox({rel:"gal",maxWidth:"100%",maxHeight:"100%",scalePhotos:true});</script><!-- fancy dates --><script>
    moment.locale("en");
    fancydates(0, "YYYY-MM-DD HH:mm");
    </script><!-- end fancy dates -->
</body>
</html>